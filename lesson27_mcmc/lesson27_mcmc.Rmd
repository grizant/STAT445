---
title: "Week 15, Lesson 27: Markov Chain Monte Carlo"
author: "AG Schissler"
date: "Monday December 3, 2018"
output: html_document
---

## I. Opening 

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
```

### Start-of-class work

1. Log in the your workstation.
2. Open the RStudio application on your machine.
3. Open lesson27_mcmc.Rmd from website so that you can work along the lecture.

### This week's agenda

Digging into more computational methods of inference and computation, including

1) Students will be able to simulate a basic Markov Chain
2) Students will be able use a Markov Chain Monte Carlo to sample a target distribution 
3) Students will be able use a Markov Chain Monte Carlo to perform Bayesian inference
4) Students will be able to use statistical computing techniques to make inferences in Lab 09

## II. Markov Chain simulation

From Braun and Murdock 2018, Markov chains are sequences of random variables $X_0, X_1, \ldots$ where the distribution of $X_{t+1}$, conditional on all previous values, depends only on $X_t$. They are commonly used to model systems with short memories, such as the stock market. When $X_t$ is a discrete random variable with a finite number of states $1,\ldots,n$, you can write the conditional distributions in an $n \times n$ matrix $P$. Where each entry $P_{ij}$ holds the conditional probability that $X_{t+1}=j$ given that $X_t = i$.

An interesting fact about Markov chains is that they have an *invariant distribution*, also called a *stationary distribution*. This is a distribution $P(X_t = i) =\pi_i$ such that if $X_t$ is drawn from the invariant distribution and updated using the transition matrix $P$, then the marginal distribution of $X_{t+1}$ will also be from the invariant distribution.

### Example: using Monte Carlo to find an invariant distribution

Consider a disease model with 3 stages. Stage 1 is healthy, Stage 2 is mild disease, and Stage 3 is severe disease. Healthy individuals remain healthy with probability 0.99 and develop mild disease with prob 0.01. Individuals with mild disease become healthy with prob 0.5, stay mild with prob 0.4, and become severely ill with prob 0.1. Finally those with severe disease stay severely sick with prob 0.75 and progress to mild status with prob 0.25.

This setting describes a Markov chain with $n=3$ states. The transition matrix is given by

$$
  P =
  \left[ {\begin{array}{ccc}
   0.99 & 0.01 & 0.00 \\
   0.50 & 0.40 & 0.10 \\
   0.00 & 0.25 & 0.75 \\
  \end{array} } \right]
$$

We'll simulate two individuals for 10000 steps: one who starts healthy and one who begins with severe disease.

```{r}
simreps <- 10000
P <- matrix(data = c(0.99, 0.01, 0, 0.5, 0.4, 0.1, 0, 0.25, 0.75), nrow = 3, byrow = TRUE)
n <- nrow(P)
X <- numeric(simreps)
## for a healthy person at time 1
X[1] <- 1

for (t in 1:(simreps-1)) {
    X[t + 1] <- sample(x = 1:n, size = 1, prob = P[X[t], ])
}

cat("Healthy person to start")
table(X)
table(X) / length(X)

## for a severly ill person at time 1
X[1] <- 3

for (t in 1:(simreps-1)) {
    X[t + 1] <- sample(x = 1:n, size = 1, prob = P[X[t], ])
}

cat("Serevely ill person to start")
table(X)
table(X) / length(X)
```

### You try

Play with different number of steps to understand short term and long term behavior in the above problem.

## III. Markov Chain Monte Carlo (MCMC) to sample a target distribution

For *some* $P$ matrices, if $X_0$ is drawn from **any** distribution, then the marginal distribution of $X_t$ for large $t$ approximates the invariant distribution. This fact is used in MCMC methods to draw values with distribution close to $\pi_i$ even when $\pi_i$ can't be calculated directly.

### The Metropolis-Hastings Algorithm (1970)

The Tale of King Markov from R. McElreath's *Statistical Rethinking*

[McElreath's slides](https://speakerdeck.com/rmcelreath/statistical-rethinking-fall-2017-lecture-10)

More formally (adapted from Rizzo 2006),

The *Metropolis-Hastings sampler* generates a Markov chain $\{X_0, X_1, \ldots \}$ as follows:

1. Choose a proposal distribution $g(\cdot | X_t)$, subject to regularity conditions discussed below.  
2. Generate $X_0$ from distribution $g$.  
3. Repeat (until chain has convergened to a stationary distribution):  
	(a) Generate $Y$ from $g(\cdot | X_t)$.  
	(b) Generate $U$ from Uniform(0,1).
	(c) If  
	$$
	U \leq \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}
	$$
accept $Y$ and set $X_{t+1} = Y$; otherwise set $X_{t+1} = X_t$.  
	(d) Increment $t$.

Observe that in step (3c) the candidate point $Y$ is accepted with probability
	$$
	\alpha (X_t, Y) = min \left( 1, \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)} \right),
	$$

So it is only necessary to know the density of the target distribution $f$ up to a constant!

The algorithm is designed so that the stationary distribution of the chain is $f$.

The proposal distribution must be chosen so that the generated chain will converge to a stationary distribution --- the target distribution. A proposal distribution with the same support as the target distribution will generally satisify regularity conditions (irreducibility, positive recurrence, and aperiodicity) to guarantee convergence.

### The Metropolis Sampler (1953)

Note that Metropolis-Hastings is a generalized of the slightly older Metropolis algorithm.

In the Metropolis algorithm, proposal distribution $g$ must be symmetric. I.e., $g(X|Y)=g(Y|X)$.

Then the acceptance ratio

$$
 r(X_t, Y) = \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}
$$
	
simplies to 

$$
 r(X_t, Y) = \frac{f(Y)}{f(X_t)}.
$$

This generalization provides greater efficiency (more rapid convergence to the stationary distribution) in certain cases.

### Example from Rizzo 2006, the Rayleigh distribution

Let's use the M-H algorithm to generate a sample from the Rayleigh distribution. The Rayleigh distribution has density

$$
f(x) = \frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)}, x \geq 0, \sigma > 0 
$$

The Rayleigh distribution is used to model lifetimes subject to rapid aging. The mode is $\sigma$, $E[X] = \sigma \sqrt{\pi / 2}$ and $Var(X) = \sigma^2 (4-\pi) /2$.

For the proposal distribution, try the chi-squared distribution with degrees of freedom of $X_t$.

In R,

```{r}
## define Rayleigh density
f <- function(x, sigma) {
    if (any(x < 0)) return(0)
    stopifnot(sigma > 0)
    return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

## construct the chain
m <- 10000
sigma <- 4
x <- numeric(m)
x[1] <- rchisq(1, df = 1)
k <- 0 ## rejections to investigate efficiency
u <- runif(m) ## acceptance probabilities

for (i in 2:m) {
    xt <- x[i-1]
    y <- rchisq(1, df = xt)
    num <- f(y, sigma) * dchisq(xt, df = y)
    den <- f(xt, sigma) * dchisq(y, df = xt)
    if (u[i] <= num/den) {
        x[i] <- y 
    } else {
        ## y is rejected
        x[i] <- xt
        k <- k + 1
    }
}

print(k) / m
```

```{r}
## go past burn-in
index <- 9000:10000
y1 <- x[index]
plot(index, y1, type = "l", main = "", ylab = "x")
```

Check the expected value.

```{r}
## expected value
mean(y1)
## in theory
sigma * sqrt(pi /2)
```

## IV. Begin working on Lab 09, time permitting

## V. Summary





